{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanzania Tourism Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tanzanian tourism sector plays a significant role in the Tanzanian economy, contributing about 17% to the country’s GDP and 25% of all foreign exchange revenues. The sector, which provides direct employment for more than 600,000 people and up to 2 million people indirectly, generated approximately $2.4 billion in 2018 according to government statistics. Tanzania received a record 1.1 million international visitor arrivals in 2014, mostly from Europe, the US and Africa.\n",
    "</br>\n",
    "</br>\n",
    "The objective of this notebook is to develop a machine learning model to predict what a tourist will spend when visiting Tanzania.The model can be used by different tour operators and the Tanzania Tourism Board to automatically help tourists across the world estimate their expenditure before visiting Tanzania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, VotingRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set the max columns to none\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# set printoptions not to display numbers in scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# set dark background for plots to be better seen in VSCode dark mode\n",
    "from matplotlib import style\n",
    "style.use('dark_background')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tanzania tourism data\n",
    "raw_data_df = pd.read_csv('data/original_zindi_data/Train.csv')\n",
    "\n",
    "# save currency conversion rate for conversion of TZS to EUR\n",
    "TZS_RATE = 2628.57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split features X and target y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw_data_df.drop(\"total_cost\", axis=1)\n",
    "y = raw_data_df[\"total_cost\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First overview of the train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a first overview of the data, the features and the target value of the training data will be examined together in a DataFrame tourism_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train features and target value for EDA\n",
    "tourism_df = pd.concat([X_train,y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourism_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourism_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourism_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are NaNs in the columns travel_with, total_female, total_male, and most_impressing.\n",
    "Most features are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourism_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing with mean of target value in test data\n",
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick overview of the distribution of the target value Total Cost of trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for target total_cost\n",
    "total_cost_bins = np.arange(tourism_df.total_cost.min(), tourism_df.total_cost.max(), 1000000)\n",
    "total_cost_bins_series = pd.cut(tourism_df.total_cost, bins=total_cost_bins, labels=total_cost_bins[:-1])\t\n",
    "total_cost_bins_series.name = 'total_cost_bins'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the target value distribution for train data\n",
    "fig, axes = plt.subplots(1,1, figsize=(30, 10))\n",
    "axes.tick_params(axis='x', rotation=90)\n",
    "fig = sns.countplot(x=total_cost_bins_series)\n",
    "\n",
    "fig.set_ylabel(\"Number of trips\", fontsize = 20)\n",
    "fig.set_xlabel(\"Total Cost of trip in TZS\", fontsize = 20)\n",
    "\n",
    "plt.savefig('images/total_cost_distribution.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the target value for total cost of a trip is right skewed. Log transformation will be used on the target data to overcome this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further examination of the categorical features and their influence on the total cost of a trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_to_total_cost(feature_column, label_name):\n",
    "    age_group_df = tourism_df.groupby(feature_column).mean()['total_cost'].reset_index().merge(tourism_df.groupby(feature_column).count()['total_cost'], on=feature_column, suffixes=('_mean', '_count'))\n",
    "    age_group_df.rename(columns={'total_cost_mean': 'mean_total_cost', 'total_cost_count': 'number_trips'}, inplace=True)\n",
    "\n",
    "    #fig, axes = plt.subplots(1,1, figsize=(15, 8))\n",
    "    plt.figure(figsize=(20,10))\n",
    "    fig = sns.scatterplot(x=feature_column, y=\"mean_total_cost\", size=\"number_trips\",\n",
    "                sizes=(40, 600), data=age_group_df)\n",
    "\n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "    fig.set_ylabel(\"Total Cost of trip in TZS\", fontsize = 20)\n",
    "    fig.set_xlabel(label_name, fontsize = 20)\n",
    "    fig.set_ylabel(label_name, fontsize = 20)\n",
    "    fig.legend_.set_title('Number of trips')\n",
    "    plt.legend(loc=2, prop={'size': 16})\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    \n",
    "    plt.savefig(f'images/{feature_column}_total_cost.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of Age Group on Total Cost of trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation of total cost to age_group\n",
    "plot_feature_to_total_cost('age_group', 'Age Group')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the age, the higher the total cost of the trip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of Purpose of travelling on Total Cost of trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation of total cost to purpose\n",
    "plot_feature_to_total_cost('purpose', 'Purpose of trip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People travelling for the purpose of Leisure and Holidays spend significantly more money than people travelling for other reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of Main Activity during travelling on Total Cost of trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation of total cost to main_activity\n",
    "plot_feature_to_total_cost('main_activity', 'Main Activity of tourists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total cost depends on main activity of the trip. The most money is spent when main activities are Diving and Sport Fishing, least for Hunting tourism and Mountain climbing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of the people who are travelling with the examined tourist on Total Cost of trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation of total cost to travel_with\n",
    "plot_feature_to_total_cost('travel_with', 'People who accompany tourists on their trip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total cost depends on who is travelled with. The most money is spent when travelling with spouse and Children."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of Payment Mode on Total Cost of trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation of total cost to payment_mode\n",
    "plot_feature_to_total_cost('payment_mode', 'Payment Mode for trip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are differences among the 4 groups of payment methods in amount of total cost. People paying with Traveller Cheques seem to spend the most money."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of Tour Arrangement on Total Cost of trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation of total cost to tour_arrangement\n",
    "plot_feature_to_total_cost('tour_arrangement', 'Tour Arrangement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significantly more money is spent when travelling with a Package Tour than when travelling Independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper examination of Tour Arrangement distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Tour arrangements\n",
    "tour_arrangement_count = tourism_df['tour_arrangement'].value_counts()\n",
    "tour_arrangement_count = tour_arrangement_count.reset_index()\n",
    "\n",
    "\n",
    "labels = list(tour_arrangement_count['index'])\n",
    "explode = (0, 0.1)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 8))\n",
    "patches, texts, autotexts  = ax.pie(tour_arrangement_count['tour_arrangement'], explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90, textprops={'color':\"black\"})\n",
    "ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "[text.set_color('white') for text in texts]\n",
    "[autotext.set_color('black') for autotext in autotexts]\n",
    "\n",
    "plt.savefig('images/tour_arrangement_percentage.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution between Independant travelling and Package Tour is almost equal, less Package Tours than Independent travelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of origin Country on Total Cost of trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation of total cost to country\n",
    "country_df = tourism_df.groupby(['country']).mean()['total_cost'].reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(1,1, figsize=(30, 10))\n",
    "fig = sns.barplot(x='country', y='total_cost', data=country_df, order = country_df.sort_values('total_cost', ascending = False).country)\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "axes.tick_params(axis='x', rotation=90)\n",
    "\n",
    "fig.set_ylabel(\"Total Cost of trip in TZS\", fontsize = 20)\n",
    "fig.set_xlabel(\"Country where tourist is coming from\", fontsize = 20)\n",
    "\n",
    "plt.savefig('images/country_total_cost.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By far the most total cost of trips is observed for people coming from Dominica and Costarica, followed by Mexico, Slovenia and Russia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of trips per origin Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of countries\n",
    "fig, axes = plt.subplots(1,1, figsize=(30, 10))\n",
    "axes.tick_params(axis='x', rotation=90)\n",
    "\n",
    "fig = sns.countplot(x = tourism_df.country,\n",
    "              order = tourism_df.country.value_counts().index)\n",
    "\n",
    "fig.set_ylabel(\"Number of trips\", fontsize = 20)\n",
    "fig.set_xlabel(\"Country where tourist is coming from\", fontsize = 20)\n",
    "\n",
    "plt.savefig('images/country_distribution.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most people going on a trip to Tantania come from the USA, UK and Italy, followed by France and Zimbabwe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examination of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of people and number of nights total\n",
    "num_cols_tourism = list(tourism_df.columns[tourism_df.dtypes!=object])\n",
    "tourism_num_cols_df = tourism_df[num_cols_tourism]\n",
    "tourism_num_cols_df.eval('total_people = total_female + total_male', inplace=True)\n",
    "tourism_num_cols_df.eval('total_nights = night_mainland + night_zanzibar', inplace=True)\n",
    "tourism_num_cols_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview of all numerical features and the target variable\n",
    "# temporarily remove outlier before plotting\n",
    "outlier_index = tourism_num_cols_df.query('total_female > 40').index\n",
    "\n",
    "sns.pairplot(tourism_num_cols_df.drop(outlier_index, axis=0)[['total_female', 'total_male','total_people', 'night_mainland', 'night_zanzibar', 'total_nights', 'total_cost']])\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "plt.savefig('images/pairplot_numerical_features.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no remarkable relation visible between the numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there is at least one person travelling\n",
    "tourism_df.query('total_female + total_male ==0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is strange, because there should be always at least one person travelling.\n",
    "Those rows will still be kept in the dataset. Besides looking at these two columns compared, the data seems to be alright. The values of those two columns won't matter that much since they are also valid when looking at them separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, the feature train dataset X_train will be treated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine missing value in column most_impressing\n",
    "X_train.most_impressing.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature can be imputed with the value \"No comments\" sine this is a valid and appropriate value for missing information on this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine missing value in column travel_with\n",
    "print(X_train.travel_with.value_counts())\n",
    "\n",
    "# percentage of missing values\n",
    "print(f'{(X_train.travel_with.isnull().sum() / X_train.shape[0] * 100).round(2)} percent of values missing')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is 23% of data missing for this feature, it is best to be imputed by a new category \"missing\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Outliers in the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find outliers with z-score\n",
    "df_zscore_total_female  = zscore(X_train.total_female.fillna(X_train.total_female.median()))\n",
    "df_zscore_total_male  = zscore(X_train.total_male.fillna(X_train.total_male.median()))\n",
    "df_zscore_night_mainland  = zscore(X_train.night_mainland.fillna(X_train.night_mainland.median()))\n",
    "df_zscore_night_zanzibar  = zscore(X_train.night_zanzibar.fillna(X_train.night_zanzibar.median()))\n",
    "\n",
    "df_scores = pd.concat([df_zscore_total_female, df_zscore_total_male, df_zscore_night_mainland, df_zscore_night_zanzibar], axis=1)\n",
    "\n",
    "# Calculate data loss based on 4 standard deviations of these 4 numeric features\n",
    "loss_percentage = df_scores.query('total_female > 4 or total_male > 4 or night_mainland > 4 or night_zanzibar > 4').count()['total_female'] / X_train.shape[0] * 100\n",
    "loss_rows = df_scores.query('total_female > 4 or total_male > 4 or night_mainland > 4 or night_zanzibar > 4')['total_female']\n",
    "\n",
    "print(f'Percentage of data loss of the training data based on 4 standard deviations for the numeric features: {loss_percentage.round(2)}%')\n",
    "print(f'Number of rows to be removed: {loss_rows.count().round(3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All outliers based on a Z-Score of 4 for the numerical features will be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers from train data\n",
    "print(loss_rows.index)\n",
    "X_train.drop(loss_rows.index, axis=0, inplace=True)\n",
    "y_train.drop(loss_rows.index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling 7 Package-columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 7 boolean features which are dependent on the outcome of the feature 'tour_arrangement'. \n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 1: Impute package columns based on tour_arrangement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are boolean columns that are only relevant for Package Tours, but they are also filled with 'No' for the Independent travellers. This might disturb the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = list(X_train.columns)\n",
    "\n",
    "# get list of all columns that only concern Package Tours\n",
    "package_columns = [col for col in all_columns if 'package' in col]\n",
    "\n",
    "package_columns\n",
    "#X_train[X_train['tour_arrangement'] == \"Independent\"].index\n",
    "\n",
    "X_train_imputed_package = X_train.copy()\n",
    "X_test_imputed_package = X_test.copy()\n",
    "# Replace values of package columns with 'Irrelevant' vor Independent travellers\n",
    "X_train_imputed_package.loc[X_train['tour_arrangement'] == 'Independent', package_columns] = 'Irrelevant'\n",
    "X_test_imputed_package.loc[X_test['tour_arrangement'] == 'Independent', package_columns] = 'Irrelevant'\n",
    "\n",
    "#TODO: This needs to go into the Pipeline!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2: Drop package columns version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a version without package columns\n",
    "X_train_without_package_cols = X_train.drop(package_columns, axis=1)\n",
    "X_test_without_package_cols = X_test.drop(package_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_without_package_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Simple Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of a significant difference of mean total_cost within the categories of age_group, purpose, main_activity, tour_arrangement and payment_mode, it is assumed that the total_cost can be predicted by taking the mean total_cost grouped by these 5 categorical features.\n",
    "Some combinations of the categories of these 5 features can be found in the test data but do not exist in the train data. Therefore, to fill in the missing mean values based on 5 features, the mean of total_cost grouped only by 2 features, age_group and tour_arrangement, is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate 5 base features and target value\n",
    "basemodel_data_train = pd.concat([X_train[['age_group','purpose','main_activity','tour_arrangement','payment_mode']],y_train], axis=1)\n",
    "\n",
    "# group data by base features and calculate mean of total_cost\n",
    "basemodel_train = basemodel_data_train.groupby(['age_group','purpose','main_activity','tour_arrangement','payment_mode']).mean()['total_cost'].reset_index()\n",
    "\n",
    "# Take easier model with only two features to impute missing values of first prediction where all 5 features in combination do not match test data\n",
    "# Concatenate 2 base features and target value\n",
    "basemodel_data_train_2features = pd.concat([X_train[['age_group','tour_arrangement']],y_train], axis=1)\n",
    "\n",
    "# group data by base features and calculate mean of total_cost\n",
    "basemodel_train_2_features = basemodel_data_train_2features.groupby(['age_group','tour_arrangement']).mean()['total_cost'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make baseline predictions for train dataset\n",
    "\n",
    "# Concatenate base features and target value\n",
    "basemodel_data_train = X_train[['age_group','purpose','main_activity','tour_arrangement','payment_mode']]\n",
    "\n",
    "# join basemodel_train to train data\n",
    "predictions_train = basemodel_data_train.merge(basemodel_train, on=['age_group','purpose','main_activity','tour_arrangement','payment_mode'], suffixes=('_test', '_train'), how='left')\n",
    "\n",
    "y_pred_train_basemodel = predictions_train['total_cost']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make baseline predictions for test dataset\n",
    "\n",
    "# Concatenate base features and target value\n",
    "basemodel_data_test = pd.concat([X_test[['age_group','purpose','main_activity','tour_arrangement','payment_mode']],y_test], axis=1)\n",
    "\n",
    "# join basemodel_train to test data\n",
    "predictions_5_features = basemodel_data_test.merge(basemodel_train, on=['age_group','purpose','main_activity','tour_arrangement','payment_mode'], suffixes=('_actual', '_5features'), how='left')\n",
    "\n",
    "# join basemodel_train_2features to predictions to fill the gaps\n",
    "predictions_5_2_features = predictions_5_features.merge(basemodel_train_2_features, on=['age_group','tour_arrangement'], how='left')\n",
    "predictions_5_2_features.rename(columns={'total_cost': 'total_cost_2features'}, inplace=True)\n",
    "\n",
    "\n",
    "# for final prediction, take prediction with 5 features and fill NaNs with prediction for 2 features\n",
    "predictions_5_2_features['total_cost_final_pred'] = predictions_5_2_features['total_cost_5features']\n",
    "predictions_5_2_features['total_cost_final_pred'] = predictions_5_2_features['total_cost_final_pred'].fillna(value = predictions_5_2_features['total_cost_2features'])\n",
    "\n",
    "y_pred_test_basemodel = predictions_5_2_features['total_cost_final_pred']\n",
    "y_act_test_basemodel = predictions_5_2_features['total_cost_actual']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: The total_cost are in TZS (Tansania-Schilling). The conversion rate is as follows: 1 EUR = 2628.57 TZN\n",
    "</br>\n",
    "RMSE is chosen as metric because it is best for the customer to see the amount of error directly in the common currency. Furthermore, because it is a squared metric, higher errors have a higher influence on the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, print_metrics=True, calculate_r2=False):\n",
    "    if calculate_r2:\n",
    "        # R2 can only be used to assess train data\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        if print_metrics:\n",
    "            print(f'R2: {r2}')\n",
    "        \n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    metrics = {'rmse' : rmse,\n",
    "                'mae' : mae}\n",
    "\n",
    "    if print_metrics:\n",
    "        print(f'RMSE in TZS: {rmse}')\n",
    "        print(f'RMSE in EUR: {rmse / TZS_RATE}')\n",
    "\n",
    "        print(f'MAE in TZS: {mae}')\n",
    "        print(f'MAE in EUR: {mae / TZS_RATE}')\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for test data\n",
    "calculate_metrics(y_act_test_basemodel,y_pred_test_basemodel, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for train data\n",
    "calculate_metrics(y_train, y_pred_train_basemodel, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: The RMSE of the baseline prediction is just a little bit better than the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error Analysis of basemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_act_vs_pred(y_true, y_pred, image_name):\n",
    "    fig, axes = plt.subplots(1,1, figsize=(15, 10))\n",
    "    fig = sns.scatterplot(y_pred, y_true)\n",
    "\n",
    "    # plot line for ideal prediction\n",
    "    x = np.linspace(0,max(y_pred),5000000)\n",
    "    y = x\n",
    "    plt.plot(x, y, '-r', label='y=x')\n",
    "\n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "    plt.ticklabel_format(style='plain', axis='x')\n",
    "    fig.set_ylabel(\"Actual values\", fontsize = 20)\n",
    "    fig.set_xlabel(\"Predicted values\", fontsize = 20)\n",
    "    plt.yticks(range(0, int(max(y_true)), 10000000))\n",
    "    plt.xticks(range(0, int(max(y_pred)), 5000000))\n",
    "\n",
    "    plt.savefig(f'images/{image_name}.png')\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs. predicted value for the baseline model\n",
    "plot_act_vs_pred(y_test, y_pred_test_basemodel, 'basemodel_act_vs_pred_values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red line shows the ideal prediction where the actual total cost would be the predicted total cost\n",
    "</br>\n",
    "The baseline model overestimates lower actual total costs and underestimates higher total costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_plot(y_test, y_pred_test, image_name, y_train=None, y_pred_train=None):\n",
    "    # convert lists to arrays\n",
    "    y_test = np.array(y_test)\n",
    "    y_pred_test = np.array(y_pred_test)\n",
    "\n",
    "    # Calculate difference of actual total_cost and predicted total_cost for test data\n",
    "    y_pred_diff = y_pred_test - y_test\n",
    "\n",
    "    # plot actual total_cost vs. difference between actual and predicted value\n",
    "    fig, axes = plt.subplots(1,1, figsize=(15, 10))\n",
    "\n",
    "    if y_train is None:\n",
    "        fig = sns.scatterplot(x=y_pred_test, y=y_pred_diff)\n",
    "        plt.xticks(range(-10000000, int(max(y_pred_test)), 10000000))\n",
    "    else:\n",
    "        y_train = np.array(y_train)\n",
    "        y_pred_train = np.array(y_pred_train)\n",
    "\n",
    "        num_test = len(y_test)\n",
    "        num_train = len(y_train)\n",
    "        y_test_total = np.concatenate((y_train, y_test))\n",
    "        y_pred_total = np.concatenate((y_pred_train, y_pred_test))\n",
    "        y_pred_diff_total = y_pred_total - y_test_total\n",
    "        train_test_num = ['train data' for i in range(num_train)] + ['test data' for i in range(num_test)]\n",
    "\n",
    "        fig = sns.scatterplot(x=y_pred_total, y=y_pred_diff_total, hue=train_test_num, alpha=0.7, palette=[\"yellow\", \"blue\"])\n",
    "        plt.xticks(range(-10000000, max(int(max(y_pred_train)), int(max(y_pred_test))), 10000000))\n",
    "\n",
    "    plt.axhline(0, c='red')\n",
    "\n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "    plt.ticklabel_format(style='plain', axis='x')\n",
    "    fig.set_xlabel(\"Predicted values\", fontsize = 20)\n",
    "    fig.set_ylabel(\"Standardized Residual\", fontsize = 20)\n",
    "\n",
    "    plt.yticks(range(-80000000, int(max(y_pred_diff)), 10000000))\n",
    "    \n",
    "    plt.savefig(f'images/{image_name}.png')\n",
    "    plt.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relation of actual value and the difference between actual and predicted values for the baseline model\n",
    "residual_plot(y_test, y_pred_test_basemodel,'basemodel_residual_plot', y_train=y_train, y_pred_train=y_pred_train_basemodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In average, for lower total_cost, the prediction is too high and for higher total_cost it is too low. \n",
    "</br>\n",
    "The error seems to be linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log transform target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transformer = FunctionTransformer(\n",
    "        func=np.log1p,\n",
    "        inverse_func=np.expm1\n",
    "    )\n",
    "\n",
    "\n",
    "y_train_log = log_transformer.transform(y_train)\n",
    "y_test_log = log_transformer.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature lists for different kinds of pipelines\n",
    "\n",
    "impute_median_features = ['total_female', 'total_male']      # num_features\n",
    "impute_missing_features = ['travel_with']                    # cat_feature\n",
    "impute_no_comments_features = ['most_impressing']            # cat_feature\n",
    "\n",
    "# ID is a unique identifier for each tourist and therefore not relevant for the model\n",
    "drop_features = ['ID']                                      # cat_feature\n",
    "\n",
    "num_features = list(X_train.columns[X_train.dtypes!=object])\n",
    "# remove items that also need to go through imputation\n",
    "num_features = [x for x in num_features if x not in impute_median_features]\n",
    "\n",
    "cat_features = list(X_train.columns[X_train.dtypes==object])\n",
    "# remove items that also need to go through imputation or need to be dropped\n",
    "cat_features = [x for x in cat_features if x not in impute_missing_features and x not in impute_no_comments_features and x not in drop_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipelines\n",
    "impute_median_pipeline = Pipeline([\n",
    "   ('imputer_num', SimpleImputer(strategy='median')),\n",
    "   ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "impute_missing_pipeline = Pipeline([\n",
    "   ('imputer_cat', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "   ('1hot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "])\n",
    "\n",
    "impute_no_comments_pipeline = Pipeline([\n",
    "   ('imputer_cat', SimpleImputer(strategy='constant', fill_value='No comments')),\n",
    "   ('1hot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "])\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "   ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "   ('1hot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "])\n",
    "\n",
    "# Create preprocessor\n",
    "# ColumnTransformer: Any columns not specified in the list of “transformers” are dropped from the dataset by default.\n",
    "preprocessor = ColumnTransformer([\n",
    "   ('median', impute_median_pipeline, impute_median_features),\n",
    "   ('missing', impute_missing_pipeline, impute_missing_features),\n",
    "   ('nocomment', impute_no_comments_pipeline, impute_no_comments_features),\n",
    "   ('num', num_pipeline, num_features),\n",
    "   ('cat', cat_pipeline, cat_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create second preprocessor for version with dropped package columns\n",
    "cat_features_wopack = [feature for feature in cat_features if feature not in package_columns]\n",
    "\n",
    "preprocessor_wopack = ColumnTransformer([\n",
    "   ('median', impute_median_pipeline, impute_median_features),\n",
    "   ('missing', impute_missing_pipeline, impute_missing_features),\n",
    "   ('nocomment', impute_no_comments_pipeline, impute_no_comments_features),\n",
    "   ('num', num_pipeline, num_features),\n",
    "   ('cat', cat_pipeline, cat_features_wopack)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_linreg = Pipeline([\n",
    "   ('preprocessor', preprocessor),\n",
    "   ('linreg', LinearRegression())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a simple Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model (without log-transformation of target)\n",
    "pipe_linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_test_linreg = pipe_linreg.predict(X_test)\n",
    "y_pred_train_linreg = pipe_linreg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for Linear Regression Model for test data\n",
    "calculate_metrics(y_test,y_pred_test_linreg, print_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for Linear Regression Model for train data\n",
    "calculate_metrics(y_train,y_pred_train_linreg, print_metrics=True, calculate_r2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs. predicted value for the model\n",
    "plot_act_vs_pred(y_test, y_pred_test_linreg, 'lin_act_vs_pred_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relation of actual value and the difference between actual and predicted value\n",
    "residual_plot(y_test, y_pred_test_linreg,'lin_residual_plot', y_train, y_pred_train_linreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model again with log-transformation of target\n",
    "pipe_linreg_log = Pipeline([\n",
    "   ('preprocessor', preprocessor),\n",
    "   ('linreg', LinearRegression())\n",
    "])\n",
    "\n",
    "pipe_linreg_log.fit(X_train, y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and inverse log-transformation\n",
    "y_pred_test_linreg_log = pipe_linreg_log.predict(X_test)\n",
    "y_pred_test_linreg_invlog = log_transformer.inverse_func(y_pred_test_linreg_log)\n",
    "\n",
    "y_pred_train_linreg_log = pipe_linreg_log.predict(X_train)\n",
    "y_pred_train_linreg_invlog = log_transformer.inverse_func(y_pred_train_linreg_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for Linear Regression Model for test data with log-transformation\n",
    "calculate_metrics(y_test,y_pred_test_linreg_invlog, print_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-transformation of the target-value made the result worse for linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick overview of performance for different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation_log(regressor, X_train, y_train, preprocessor = preprocessor, log=False, version=''):\n",
    "    if version == 'wopack':\n",
    "        pipe_regressor = Pipeline([\n",
    "            ('preprocessor', preprocessor_wopack),\n",
    "            ('regressor', regressor)\n",
    "        ])\n",
    "    else:\n",
    "        pipe_regressor = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', regressor)\n",
    "        ])\n",
    "\n",
    "    y_pred = cross_val_predict(pipe_regressor, X_train, y_train, cv=10, n_jobs=-1)\n",
    "\n",
    "    results = {}\n",
    "    if log:\n",
    "        y_pred = log_transformer.inverse_func(y_pred)\n",
    "\n",
    "    results['model'] = regressor\n",
    "    results['rmse'] = mean_squared_error(y_train, y_pred, squared=False)\n",
    "    results['mae'] = mean_absolute_error(y_train, y_pred)\n",
    "    results['version'] = 'normal' + version if not log else 'log' + version\n",
    "    results['y_pred'] = y_pred\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate different models to make the decision for the best of them to be further adjusted\n",
    "list_of_estimators = [('lr', LinearRegression()),\n",
    "                     ('dt', DecisionTreeRegressor(random_state=42)),\n",
    "                     ('svm', SVR()),\n",
    "                     ('knn', KNeighborsRegressor()),\n",
    "                     ('rf', RandomForestRegressor())]\n",
    "\n",
    "stacking_final_estimator = RandomForestRegressor()\n",
    "\n",
    "list_of_reg = [LinearRegression(), \n",
    "               DecisionTreeRegressor(random_state=42), \n",
    "               SVR(), \n",
    "               KNeighborsRegressor(), \n",
    "               RandomForestRegressor(random_state=42), \n",
    "               AdaBoostRegressor(random_state=42), \n",
    "               XGBRegressor(random_state=42), \n",
    "               VotingRegressor(estimators=list_of_estimators), \n",
    "               StackingRegressor(estimators=list_of_estimators, final_estimator=stacking_final_estimator)]#,\n",
    "               #StackingRegressor(estimators=list_of_estimators, final_estimator=VotingRegressor(estimators=list_of_estimators))]\n",
    " \n",
    "model_results = []\n",
    "model_results_log = []\n",
    "model_results_imp = []\n",
    "model_results_imp_log = []\n",
    "model_results_wopack = []\n",
    "model_results_wopack_log = []\n",
    "\n",
    "for reg in list_of_reg:\n",
    "   results = model_evaluation_log(reg, X_train, y_train, preprocessor, log=False)\n",
    "   results_log = model_evaluation_log(reg, X_train, y_train_log, preprocessor, log=True)\n",
    "\n",
    "   print(results['model'])\n",
    "   print(f\"RSME: {results['rmse']}\")\n",
    "   print(f\"MAE: {results['mae']}\")\n",
    "   print(f\"RSME log: {results_log['rmse']}\")\n",
    "   print(f\"MAE log: {results_log['mae']}\")\n",
    "\n",
    "   results_imp = model_evaluation_log(reg, X_train_imputed_package, y_train, preprocessor, log=False, version='imp')\n",
    "   results_imp_log = model_evaluation_log(reg, X_train_imputed_package, y_train_log, preprocessor, log=True, version='imp')\n",
    "\n",
    "   print(f\"RSME imp: {results_imp['rmse']}\")\n",
    "   print(f\"MAE imp: {results_imp['mae']}\")\n",
    "   print(f\"RSME imp log: {results_imp_log['rmse']}\")\n",
    "   print(f\"MAE imp log: {results_imp_log['mae']}\")\n",
    "   \n",
    "   results_wopack = model_evaluation_log(reg, X_train_without_package_cols, y_train, preprocessor, log=False, version='wopack')\n",
    "   results_wopack_log = model_evaluation_log(reg, X_train_without_package_cols, y_train_log, preprocessor, log=True, version='wopack')\n",
    "\n",
    "   print(f\"RSME wopack: {results_wopack['rmse']}\")\n",
    "   print(f\"MAE wopack: {results_wopack['mae']}\")\n",
    "   print(f\"RSME wopack log: {results_wopack_log['rmse']}\")\n",
    "   print(f\"MAE wopack log: {results_wopack_log['mae']}\")\n",
    "\n",
    "   print(\"----\"*10)\n",
    "\n",
    "   model_results.append(results)\n",
    "   model_results_log.append(results_log)\n",
    "\n",
    "   model_results_imp.append(results_imp)\n",
    "   model_results_imp_log.append(results_imp_log)\n",
    "\n",
    "   model_results_wopack.append(results_wopack)\n",
    "   model_results_wopack_log.append(results_wopack_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank results of the different models by RMSE mean\n",
    "model_results_df = pd.DataFrame.from_dict(model_results)\n",
    "model_results_log_df = pd.DataFrame.from_dict(model_results_log)\n",
    "model_results_imp_df = pd.DataFrame.from_dict(model_results_imp)\n",
    "model_results_imp_log_df = pd.DataFrame.from_dict(model_results_imp_log)\n",
    "model_results_wopack_df = pd.DataFrame.from_dict(model_results_wopack)\n",
    "model_results_wopack_log_df = pd.DataFrame.from_dict(model_results_wopack_log)\n",
    "\n",
    "model_results_total_df = pd.concat([model_results_df, model_results_log_df, model_results_imp_df, model_results_imp_log_df, model_results_wopack_df, model_results_wopack_log_df], axis=0, ignore_index=True)\n",
    "\n",
    "model_results_total_df.sort_values('rmse')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All regressors perform better on log-transformed data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plot AdaBoost (log wopack)\n",
    "residual_plot(y_train, model_results_total_df.loc[50, 'y_pred'],'1_adaboost_logwopack_residual_plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train AdaBoost model with GridSearch for log-transformed data without package columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized Grid Search for AdaBoostRegressor\n",
    "param_grid = {#\"regressor__base_estimator\": [DecisionTreeRegressor()], # LinearRegression()], KNeighborsRegressor()],\n",
    "            'regressor__n_estimators': [10, 30, 50, 70, 90],\n",
    "            'regressor__learning_rate': [0.3, 0.5, 0.7, 1.0],\n",
    "            'regressor__loss': ['linear', 'square', 'exponential'],\n",
    "            'regressor__base_estimator__max_depth': np.arange(5,70,5),\n",
    "            'regressor__base_estimator__min_samples_split': np.arange(5,30, 5),\n",
    "            'regressor__base_estimator__splitter': ['best', 'random']\n",
    "            }\n",
    "\n",
    "dtreg = DecisionTreeRegressor(criterion='squared_error')\n",
    "\n",
    "pipe_regressor = Pipeline([\n",
    "        ('preprocessor', preprocessor_wopack),      #preprocessor\n",
    "        ('regressor', AdaBoostRegressor(base_estimator= dtreg, random_state=42))\n",
    "    ])\n",
    "\n",
    "rand_search = RandomizedSearchCV(pipe_regressor, param_distributions=param_grid, scoring='neg_root_mean_squared_error', cv=5, verbose=1, n_jobs=-1, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model with Adaboost, without the package columns and with log-transformed target column\n",
    "rand_search.fit(X_train_without_package_cols, y_train_log)          #X_train_imputed_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rand_search.best_score_ * (-1.0))\n",
    "print(rand_search.best_params_)\n",
    "#print(gs.best_estimator_)\n",
    "\n",
    "# save best model\n",
    "best_model_ada = rand_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_ada_rs_log = best_model_ada.predict(X_test_without_package_cols)\n",
    "y_test_pred_ada_rs = log_transformer.inverse_func(y_test_pred_ada_rs_log)\n",
    "\n",
    "y_train_pred_ada_rs_log = best_model_ada.predict(X_train_without_package_cols)\n",
    "y_train_pred_ada_rs = log_transformer.inverse_func(y_train_pred_ada_rs_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(y_test, y_test_pred_ada_rs, print_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(y_train, y_train_pred_ada_rs, print_metrics=True, calculate_r2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs. predicted value for the model\n",
    "plot_act_vs_pred(y_test, y_test_pred_ada_rs, 'ada_randsearch_act_vs_pred_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relation of actual value and the difference between actual and predicted value\n",
    "residual_plot(y_test, y_test_pred_ada_rs,'ada_randsearch_residual_plot', y_train, y_train_pred_ada_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis - Deep dive into difference of high and low total cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare travellers with high to low total_cost\n",
    "train_data_all_df = pd.concat([X_train,y_train], axis=1)\n",
    "\n",
    "# Divide travellers in those who have high total_cost and those with low total_cost\n",
    "low_cost_df = train_data_all_df.query('total_cost <= 10000000')  #2818 rows\n",
    "high_cost_df = train_data_all_df.query('total_cost > 10000000')  #913 rows\n",
    "\n",
    "no_low = low_cost_df.shape[0]\n",
    "no_high = high_cost_df.shape[0]\n",
    "\n",
    "# Create feature groups\n",
    "# Country not included because there are too many categories in this feature\n",
    "percentage_grouped_low = low_cost_df.groupby(['main_activity', 'age_group', 'travel_with', 'purpose', 'payment_mode', 'info_source', 'tour_arrangement', 'first_trip_tz', 'country','most_impressing']).count()['ID'] / no_low\n",
    "percentage_grouped_low = percentage_grouped_low.reset_index()\n",
    "percentage_grouped_high = high_cost_df.groupby(['main_activity', 'age_group', 'travel_with', 'purpose', 'payment_mode', 'info_source', 'tour_arrangement', 'first_trip_tz', 'country','most_impressing']).count()['ID'] / no_high\n",
    "percentage_grouped_high = percentage_grouped_high.reset_index()\n",
    "\n",
    "# join basemodel_train to test data\n",
    "percentage_total = percentage_grouped_low.merge(percentage_grouped_high, on=['main_activity', 'age_group', 'travel_with', 'purpose', 'payment_mode', 'info_source', 'tour_arrangement', 'first_trip_tz', 'country','most_impressing'], suffixes=('_low', '_high'), how='outer')\n",
    "percentage_total.rename(columns={'ID_low': 'percentage_low_cost', 'ID_high': 'percentage_high_cost'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "percentage_total.sort_values('tour_arrangement').head(30)\n",
    "\n",
    "#percentage_total.shape[0] # 2285# feature groups\n",
    "\n",
    "percentage_total.isnull().sum()\n",
    "# 580 of total groups do not exist for low total_cost\n",
    "# 1625 of total groups do not exist for high total_cost\n",
    "# --> not enough data to represent the high total_cost group for some feature-combinations!!\n",
    "\n",
    "#percentage_total[percentage_total['percentage_high_cost'].notnull()]\n",
    "\n",
    "#percentage_total.sort_values('percentage_high_cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_high = [580, 1625, int(percentage_total.shape[0])]\n",
    "labels = ['missing for trips with low cost', 'missing for trips with high cost', 'Total possible combinations']\n",
    "low_high\n",
    "\n",
    "fig, axes = plt.subplots(1,1, figsize=(10, 5))\n",
    "fig = sns.barplot(labels, low_high)\n",
    "\n",
    "\n",
    "for p in fig.patches:\n",
    "    fig.annotate(format(p.get_height(), '.1f'), \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   size=15,\n",
    "                   color='Black',\n",
    "                   xytext = (0, -12), \n",
    "                   textcoords = 'offset points')\n",
    "\n",
    "plt.savefig('images/missing_data_low_vs_high_cost.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_high = [low_cost_df.shape[0], high_cost_df.shape[0]]\n",
    "labels = ['Number of trips with low cost', 'Number of trips with high cost']\n",
    "low_high\n",
    "\n",
    "fig, axes = plt.subplots(1,1, figsize=(7, 10))\n",
    "fig = sns.barplot(labels, low_high)\n",
    "\n",
    "for p in fig.patches:\n",
    "    fig.annotate(format(p.get_height(), '.1f'), \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   size=15,\n",
    "                   color='Black',\n",
    "                   xytext = (0, -12), \n",
    "                   textcoords = 'offset points')\n",
    "\n",
    "plt.savefig('images/number_trips_low_vs_high_cost.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the group of travellers with high total_cost, it seems that they are only centered in a few groups of feature-combinations.\n",
    "</br>\n",
    "The dataset is too small to be assessed by these many features.\n",
    "</br>\n",
    "More data is needed!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
