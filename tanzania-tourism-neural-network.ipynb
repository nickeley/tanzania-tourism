{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35455b7",
   "metadata": {},
   "source": [
    "# Using a Neural Network for Tanzania Tourism Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "    \n",
    "print('Using TensorFlow version: %s' % tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b25ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q git+https://github.com/tensorflow/docs\n",
    "    \n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a12ef62",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb56623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "raw_data_df = pd.read_csv('data/original_zindi_data/Train.csv')\n",
    "\n",
    "#cleaning data and preparing\n",
    "X = raw_data_df.drop(\"total_cost\", axis=1)\n",
    "y = raw_data_df[\"total_cost\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f17c4",
   "metadata": {},
   "source": [
    "# Preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7afbd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "def create_preprocessor(X_train:pd.DataFrame):\n",
    "    print(f\"Create preprocessor\")\n",
    "    \n",
    "    # Create feature lists for different kinds of pipelines\n",
    "    impute_median_features = ['total_female', 'total_male']      # num_features\n",
    "    impute_missing_features = ['travel_with']                    # cat_feature\n",
    "    impute_no_comments_features = ['most_impressing']            # cat_feature\n",
    "\n",
    "    # ID is a unique identifier for each tourist and therefore not relevant for the model\n",
    "    drop_features = ['ID']                                      # cat_feature\n",
    "\n",
    "    num_features = list(X_train.columns[X_train.dtypes!=object])\n",
    "    # remove items that also need to go through imputation\n",
    "    num_features = [x for x in num_features if x not in impute_median_features]\n",
    "\n",
    "    cat_features = list(X_train.columns[X_train.dtypes==object])\n",
    "\n",
    "    all_columns = list(X.columns)\n",
    "    # get list of all columns that only concern Package Tours\n",
    "    package_columns = [col for col in all_columns if 'package' in col]\n",
    "\n",
    "    # remove items that also need to go through imputation or need to be dropped and remove package columns\n",
    "    cat_features = [x for x in cat_features if x not in impute_missing_features and x not in impute_no_comments_features and x not in drop_features and x not in package_columns]\n",
    "\n",
    "    # Create preprocessing pipelines\n",
    "    impute_median_pipeline = Pipeline([\n",
    "    ('imputer_num', SimpleImputer(strategy='median')),\n",
    "    ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    impute_missing_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('1hot', OneHotEncoder(handle_unknown='ignore', drop='first', sparse=False))\n",
    "    ])\n",
    "\n",
    "    impute_no_comments_pipeline = Pipeline([\n",
    "    ('imputer_cat', SimpleImputer(strategy='constant', fill_value='No comments')),\n",
    "    ('1hot', OneHotEncoder(handle_unknown='ignore', drop='first', sparse=False))\n",
    "    ])\n",
    "\n",
    "    num_pipeline = Pipeline([\n",
    "    ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    cat_pipeline = Pipeline([\n",
    "    ('1hot', OneHotEncoder(handle_unknown='ignore', drop='first', sparse=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('median', impute_median_pipeline, impute_median_features),\n",
    "        ('missing', impute_missing_pipeline, impute_missing_features),\n",
    "        ('nocomment', impute_no_comments_pipeline, impute_no_comments_features),\n",
    "        ('num', num_pipeline, num_features),\n",
    "        ('cat', cat_pipeline, cat_features)\n",
    "        ])\n",
    "\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d9c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a preprocessor\n",
    "preprocessor = create_preprocessor(X_train)\n",
    "\n",
    "# Use the preprocessor to preprocess the data\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8bf465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for log-transforming of target (y)\n",
    "def create_log_transformer():\n",
    "    return FunctionTransformer(\n",
    "        func=np.log1p,\n",
    "        inverse_func=np.expm1\n",
    "    )\n",
    "\n",
    "def log_transform_target(y:pd.Series) -> pd.Series:\n",
    "    print(\"Log-transform y\")\n",
    "    log_transformer = create_log_transformer()\n",
    "    y_log = log_transformer.transform(y)\n",
    "    return y_log\n",
    "\n",
    "def inverse_log_transform_target(y_log:pd.Series) -> pd.Series:\n",
    "    print(\"Inverse-log-transform y\")\n",
    "    log_transformer = create_log_transformer()\n",
    "    y = log_transformer.inverse_func(y_log)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b6245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-transformation of data\n",
    "y_train_log = log_transform_target(y_train)\n",
    "y_test_log = log_transform_target(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26098865",
   "metadata": {},
   "source": [
    "## Preparation for logging with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "N_TRAIN = len(X_train)\n",
    "BATCH_SIZE = 32\n",
    "STEPS_PER_EPOCH = N_TRAIN // BATCH_SIZE\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d9f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure checkpoints for saving model in between calculation\n",
    "checkpoint_path = \"training__tanzania/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True,verbose=0)\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs_tanzania\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directory for logging\n",
    "def get_run_logdir():\n",
    "   run_id = time.strftime('run_%d_%m_%Y-%H_%M_%S')\n",
    "   return os.path.join(root_logdir, run_id)\n",
    "\n",
    "\n",
    "run_logdir = get_run_logdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a2e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks for logging to use in Tensorboard\n",
    "def get_callbacks(name):\n",
    "# returns list of callbacks\n",
    "  return [\n",
    "    tfdocs.modeling.EpochDots(),    # to reduce logging noise\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200),\n",
    "    tf.keras.callbacks.TensorBoard(run_logdir+name, histogram_freq=1)   # to produce logs for using Tensorboard\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5a986a",
   "metadata": {},
   "source": [
    "# Train Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b62b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model name for logging\n",
    "your_history = {}\n",
    "model_name = 'first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_compile_and_fit(model, steps_per_epoch, epochs, batch_size, model_name): \n",
    "    # learning rate schedule\n",
    "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "        0.01,\n",
    "        decay_steps=steps_per_epoch*1000,\n",
    "        decay_rate=1,\n",
    "        staircase=False)\n",
    "\n",
    "    # Get optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, name='Adam')\n",
    "\n",
    "    # model.compile\n",
    "    with tf.device('/cpu:0'):\t\t#optional, only for mac!!\n",
    "        model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mae',\n",
    "        metrics='mse')\n",
    "\n",
    "    # model.fit\n",
    "    # with preprocessed features and log-transformed target variable\n",
    "    with tf.device('/cpu:0'):\n",
    "        results = model.fit(X_train_preprocessed,\n",
    "                            y_train_log,\n",
    "                            validation_split=0.2,\n",
    "                            verbose=0,\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            callbacks=get_callbacks(model_name)\n",
    "                            )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b436f486",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63515787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define normalizer\n",
    "normalizer = preprocessing.Normalization(name='norm', input_shape=[(X_train_preprocessed).shape[1]],axis = None)\n",
    "normalizer.adapt(np.array(X_train_preprocessed))\n",
    "normalizer.mean.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535bfd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model_tanzania = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(name='layer1', input_shape = [None, (X_train_preprocessed).shape[1]], units=64, activation='relu'),\n",
    "    layers.Dense(name='layer2', units=1)\n",
    "    ]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31056e5",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df56c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_history['first_model'] = model_compile_and_fit(model_tanzania, STEPS_PER_EPOCH, EPOCHS, BATCH_SIZE, 'first_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a0e282",
   "metadata": {},
   "source": [
    "## Evaluate the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plotter = tfdocs.plots.HistoryPlotter(metric = 'mse', smoothing_std=10)\n",
    "history_plotter.plot(your_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252efa0",
   "metadata": {},
   "source": [
    "## Load Tensorboard\n",
    "(execute the next cell twice if it does not work the first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276fd64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=./my_logs_tanzania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85938cce",
   "metadata": {},
   "source": [
    "## Further model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3acc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dropout layer\n",
    "# instantiate model\n",
    "model_tanzania = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(name='layer1', input_shape = [None, (X_train_preprocessed).shape[1]], units=64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(name='layer2', units=1)\n",
    "    ]) \n",
    "\n",
    "your_history['dropout_reg_model'] = model_compile_and_fit(model_tanzania, STEPS_PER_EPOCH, EPOCHS, BATCH_SIZE, 'dropout_reg_model')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5d7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plotter = tfdocs.plots.HistoryPlotter(metric = 'mse', smoothing_std=10)\n",
    "history_plotter.plot(your_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01526db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more dense layers\n",
    "# instantiate model\n",
    "model_tanzania = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(name='layer1', input_shape = [None, (X_train_preprocessed).shape[1]], units=64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(name='layer2', input_shape = [None, (X_train_preprocessed).shape[1]], units=64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(name='layer3', units=1)\n",
    "    ]) \n",
    "\n",
    "your_history['more_layers_model'] = model_compile_and_fit(model_tanzania, STEPS_PER_EPOCH, EPOCHS, BATCH_SIZE, 'more_layers_model')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b940e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plotter = tfdocs.plots.HistoryPlotter(metric = 'mse', smoothing_std=10)\n",
    "history_plotter.plot(your_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
